{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from sklearn import preprocessing\n",
    "from sklearn import impute\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in training data\n",
    "raw_data = pd.read_csv('./data/movementSensorData.csv')\n",
    "\n",
    "time = raw_data.iloc[:, 2] # just time\n",
    "data = raw_data.iloc[:, 3:5] #data minus activity\n",
    "labels = raw_data.iloc[:, 1] #just activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing if necessary\n",
    "#Impute missing data using K Nearest Neighbours (n=5)\n",
    "#imputer = impute.KNNImputer(missing_values=np.nan, n_neighbors=5)\n",
    "#imputed_data = pd.DataFrame(imputer.fit_transform(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaliser = preprocessing.Normalizer()\n",
    "data = pd.DataFrame(normaliser.fit_transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale std and mean\n",
    "scaler = preprocessing.StandardScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection if necessary\n",
    "#selector = VarianceThreshold()\n",
    "#feature_selected_data = selector.fit_transform(scaled_data, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time clustering (group together multiple datapoints in the same timeframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to nicely present findings\n",
    "def print_metrics(truth, predictions):\n",
    "  metrics.ConfusionMatrixDisplay.from_predictions(truth, predictions)\n",
    "  print('Accuracy', metrics.accuracy_score(truth, predictions))\n",
    "  print('Precision', metrics.precision_score(truth, predictions))\n",
    "  print('Recall', metrics.recall_score(truth, predictions))\n",
    "  print('F1 Score', metrics.f1_score(truth, predictions))\n",
    "\n",
    "def plot_classifiers_figure(train_data, test_data, train_labels, test_labels, names, classifiers):\n",
    "  fig = plt.figure(figsize=(10,5))\n",
    "  ax = fig.add_axes([0,0,1,1])\n",
    "  accuracy, precision, recall, f1 = [],[],[],[]\n",
    "  for name, clf in zip(names, classifiers):\n",
    "    clf.fit(train_data, train_labels)\n",
    "    predictions = clf.predict(test_data)\n",
    "\n",
    "    accuracy.append(metrics.accuracy_score(test_labels, predictions))\n",
    "    precision.append(metrics.precision_score(test_labels, predictions, average='weighted'))\n",
    "    recall.append(metrics.recall_score(test_labels, predictions, average='weighted'))\n",
    "    f1.append(metrics.f1_score(test_labels, predictions, average='weighted'))\n",
    "\n",
    "  x = np.arange(len(classifiers))\n",
    "  ax.bar(x-0.30, accuracy, color='r', width=0.20, label='Accuracy')\n",
    "  ax.bar(x-0.10, precision, color='b', width=0.20, label='Precision')\n",
    "  ax.bar(x+0.10, recall, color='g', width=0.20, label='Recall')\n",
    "  ax.bar(x+0.30, f1, color = 'y', width=0.20, label='F1 Score')\n",
    "\n",
    "  plt.xticks(x, names, rotation=15)\n",
    "  ax.set_yticks(np.arange(0, 1.05, 0.05))\n",
    "  ax.legend(title=\"Metrics\", loc='upper left')\n",
    "  ax.grid(alpha=0.5, axis='y')\n",
    "  plt.show()\n",
    "\n",
    "def plot_data_figure(train_data, test_data, train_labels, test_labels, classifier):\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_axes([0,0,1,1])\n",
    "  accuracy, precision, recall, f1 = [],[],[],[]\n",
    "  j = 200\n",
    "  for i in range(500, 3001, 500):\n",
    "    classifier.fit(train_data[:i], train_labels[:i])\n",
    "    predictions = classifier.predict(test_data[:j])\n",
    "\n",
    "    accuracy.append(metrics.accuracy_score(test_labels[:j], predictions))\n",
    "    precision.append(metrics.precision_score(test_labels[:j], predictions, average='weighted'))\n",
    "    recall.append(metrics.recall_score(test_labels[:j], predictions, average='weighted'))\n",
    "    f1.append(metrics.f1_score(test_labels[:j], predictions, average='weighted'))\n",
    "    j += 200\n",
    "\n",
    "  x = np.arange(len(accuracy))\n",
    "  ax.bar(x-0.30, accuracy, color='r', width=0.20, label='Accuracy')\n",
    "  ax.bar(x-0.10, precision, color='b', width=0.20, label='Precision')\n",
    "  ax.bar(x+0.10, recall, color='g', width=0.20, label='Recall')\n",
    "  ax.bar(x+0.30, f1, color = 'y', width=0.20, label='F1 Score')\n",
    "\n",
    "  plt.xticks(x, [i for i in range(500, 3001, 500)], rotation=15)\n",
    "  ax.set_yticks(np.arange(0, 1.05, 0.05))\n",
    "  ax.legend(title=\"Metrics\", loc='upper left')\n",
    "  ax.grid(alpha=0.5, axis='y')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata, vdata, tlabels, vlabels = train_test_split(data, labels, test_size=0.2, shuffle=True, random_state=1452) #get train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Multi Layer Perceptron', 'Tuned Multi Layer Perceptron',\n",
    "         'Logistic Regression', 'Tuned Logistic Regression', 'Source Vector Classifier', 'Tuned Source Vector Classifier']\n",
    "classifiers = [\n",
    "  MLPClassifier(),\n",
    "  MLPClassifier(hidden_layer_sizes=(25,), activation='relu', solver='adam', learning_rate='adaptive', alpha=0.001, batch_size=100),\n",
    "  LogisticRegression(max_iter=1000), #had to up max iter or it wouldn't converge\n",
    "  LogisticRegression(solver='newton-cg', fit_intercept=False),\n",
    "  SVC(),\n",
    "  SVC(kernel='sigmoid'),\n",
    "  RandomForestClassifier(),\n",
    "  LinearRegression()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "plot_classifiers_figure(tdata, vdata, tlabels, vlabels, names, classifiers) #Plot a comparison of my selected classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_pred.to_csv(filepath + 'predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
